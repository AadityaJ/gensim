{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using wrappers for Scikit learn API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is about using gensim models as a part of your scikit learn workflow with the help of wrappers found at ```gensim.sklearn_integration.SklearnWrapperGensimLdaModel```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapper available (as of now) are :\n",
    "* LdaModel (```gensim.sklearn_integration.SklearnWrapperGensimLdaModel.SklearnWrapperLdaModel```),which implements gensim's ```LdaModel``` in a scikit-learn interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LdaModel begin with importing LdaModel wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.sklearn_integration.SklearnWrapperGensimLdaModel import SklearnWrapperLdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create a dummy set of texts and convert it into a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "texts = [['complier', 'system', 'computer'],\n",
    " ['eulerian', 'node', 'cycle', 'graph', 'tree', 'path'],\n",
    " ['graph', 'flow', 'network', 'graph'],\n",
    " ['loading', 'computer', 'system'],\n",
    " ['user', 'server', 'system'],\n",
    " ['tree','hamiltonian'],\n",
    " ['graph', 'trees'],\n",
    " ['computer', 'kernel', 'malfunction','computer'],\n",
    " ['server','system','computer']]\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to run the LdaModel on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.164*computer + 0.117*system + 0.105*graph + 0.061*server + 0.057*tree + 0.046*malfunction + 0.045*kernel + 0.045*complier + 0.043*loading + 0.039*hamiltonian'),\n",
       " (1,\n",
       "  u'0.102*graph + 0.083*system + 0.072*tree + 0.064*server + 0.059*user + 0.059*computer + 0.057*trees + 0.056*eulerian + 0.055*node + 0.052*flow')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=SklearnWrapperLdaModel(num_topics=2,id2word=dictionary,iterations=20, random_state=1)\n",
    "model.fit(corpus)\n",
    "model.print_topics(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Integration with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide a better example of how it can be used with Sklearn, Let's use CountVectorizer method of sklearn. For this example we will use [20 Newsgroups data set](http://qwone.com/~jason/20Newsgroups/). We will only use the categories rec.sport.baseball and sci.crypt and use it to generate topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import matutils\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.sklearn_integration.SklearnWrapperGensimLdaModel import SklearnWrapperLdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand = np.random.mtrand.RandomState(1) # set seed for getting same result\n",
    "cats = ['rec.sport.baseball', 'sci.crypt']\n",
    "data = fetch_20newsgroups(subset='train',\n",
    "                        categories=cats,\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use countvectorizer to convert the collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(min_df=10, stop_words='english')\n",
    "\n",
    "X = vec.fit_transform(data.data)\n",
    "vocab = vec.get_feature_names() #vocab to be converted to id2word \n",
    "\n",
    "id2word=dict([(i, s) for i, s in enumerate(vocab)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we just need to fit X and id2word to our Lda wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.031*fierkelab + 0.025*bitnet + 0.020*digex + 0.020*false + 0.018*cover + 0.014*disk + 0.014*fear + 0.013*effort + 0.012*amolitor + 0.012*brian'),\n",
       " (1,\n",
       "  u'0.022*corporate + 0.017*accurate + 0.015*chance + 0.008*dawson + 0.007*assess + 0.007*afford + 0.006*administration + 0.006*denning + 0.006*broad + 0.006*fails'),\n",
       " (2,\n",
       "  u'0.018*clark + 0.017*accurate + 0.014*decipher + 0.014*example + 0.013*authentication + 0.012*cases + 0.012*follow + 0.011*basically + 0.011*candidates + 0.011*decryption'),\n",
       " (3,\n",
       "  u'0.116*abroad + 0.093*asking + 0.078*cryptography + 0.040*arithmetic + 0.033*argue + 0.033*ciphertext + 0.031*courtesy + 0.028*456 + 0.028*facts + 0.020*beastmaster'),\n",
       " (4,\n",
       "  u'0.026*certain + 0.020*book + 0.020*69 + 0.019*demand + 0.019*87 + 0.019*cracking + 0.018*farm + 0.012*face + 0.011*constitutional + 0.009*cryptography')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj=SklearnWrapperLdaModel(id2word=id2word,num_topics=5,passes=20)\n",
    "lda=obj.fit_predict(X)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
